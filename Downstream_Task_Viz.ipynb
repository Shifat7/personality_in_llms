{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lwi5X93H4-f1",
      "metadata": {
        "id": "lwi5X93H4-f1"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "This colab contains code used to generate the wordcloud visualization for the Real-World Task experiments (generating social media updates) on the various LLMs experimented on in the paper \"Personality Traits in Large Language Models\" (https://arxiv.org/pdf/2307.00184). The code uses the \"scored\" LLM outputs (pickled dataframes) from the Independent Trait Shaping Analysis notebook, as well as the actual social media updates generated by the LLMs (in their own pickled df), along with the AMS scores (CSV file) resulting from running the Apply Magic Sauce model on the generated updates. The code below assumes that all the data produced and consumed in the colab (especially the pickled dataframe outputs of running inference on various LLMs) lives in a local filesystem either in a cloud instance running a Jupyter notebook such as Google Colab or a desktop. But those file I/O operations can easily be replaced to use any other file management solutions. The inline comments for some of the operations explain the motivation behind them and what to expect in the results of running an analysis in a cell.\n",
        "\n",
        "To run this colab:\n",
        "1. Connect to an appropriate runtime. (For instance, if running the bulk inference directly from the colab, connect to a GPU kernel.)\n",
        "2. Check experiment parameters below.\n",
        "3. Run the code cells for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1146867a-9b04-4327-86ee-b77922dabf5b",
      "metadata": {
        "id": "1146867a-9b04-4327-86ee-b77922dabf5b"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cbdc6ce-b584-4b6a-9870-b85ae9974924",
      "metadata": {
        "id": "2cbdc6ce-b584-4b6a-9870-b85ae9974924"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eagRTAEi7qvb",
      "metadata": {
        "id": "eagRTAEi7qvb"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies for Downstream task analysis and visualization\n",
        "%pip install pandas\n",
        "%pip install matplotlib\n",
        "%pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c70fea3-e540-4a05-9ec3-62e724644326",
      "metadata": {
        "id": "6c70fea3-e540-4a05-9ec3-62e724644326"
      },
      "outputs": [],
      "source": [
        "#@title Load dependencies\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vp2caJdAGy3s",
      "metadata": {
        "id": "vp2caJdAGy3s"
      },
      "outputs": [],
      "source": [
        "#@title File locations setup  { run: \"auto\" }\n",
        "\n",
        "#@markdown `ABL01_SCORES` is the filename of pickled results of the output from\n",
        "#@markdown the Independent Trait Shaping Analysis notebook.\n",
        "#@markdown This \"scored session dataframe \"is an input for this colab.\n",
        "ABL01_SCORES = 'sample_pkl_file.pkl'  # @param {\"type\":\"string\"}\n",
        "\n",
        "#@markdown `STATUS_UPDATES_DATA` is the file path of the output from the LLMs\n",
        "#@markdown generating the downstream task (social media updates) data based on\n",
        "#@markdown their shaped traits.\n",
        "STATUS_UPDATES_DATA = 'sample_downstream_data.pkl'  # @param {\"type\":\"string\"}\n",
        "\n",
        "#@markdown `AMS_DATA` is the path to the results of running AMS on the file above.\n",
        "AMS_DATA = 'sample_ams_output.csv'  # @param {\"type\":\"string\"}\n",
        "\n",
        "#@markdown Path of the file where the dataframe created by joining the above three\n",
        "#@markdown pieces of data can be stored as a pickled dataframe.\n",
        "SAVE_SCORES_FILENAME = 'sample_scored_dataframe.pkl'  # @param {\"type\":\"string\"}\n",
        "\n",
        "#@markdown Whether the model who's data is being analyzed is a PaLM model variant or not?\n",
        "#@markdown Some of the pre-processing on the input dataframe differs between PaLM and non-PaLM models.\n",
        "IS_PALM_MODEL = True  # @param {\"type\":\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa91294c-7acf-4249-b143-3127aa99aca5",
      "metadata": {
        "id": "fa91294c-7acf-4249-b143-3127aa99aca5"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172411ce-f8f9-4a1b-88d1-ac6416316cdf",
      "metadata": {
        "id": "172411ce-f8f9-4a1b-88d1-ac6416316cdf"
      },
      "outputs": [],
      "source": [
        "IPIP_SCALE_IDS = [\n",
        "    'IPIP300-EXT',\n",
        "    'IPIP300-AGR',\n",
        "    'IPIP300-CON',\n",
        "    'IPIP300-NEU',\n",
        "    'IPIP300-OPE'\n",
        "]\n",
        "\n",
        "AMS_SCALE_IDS = [\n",
        "    'ams-IPIP300-EXT',\n",
        "    'ams-IPIP300-AGR',\n",
        "    'ams-IPIP300-CON',\n",
        "    'ams-IPIP300-NEU',\n",
        "    'ams-IPIP300-OPE'\n",
        "]\n",
        "\n",
        "AMS_SCALE_IDS_2 = [\n",
        "    'BIG5_Extraversion',\n",
        "    'BIG5_Agreeableness',\n",
        "    'BIG5_Conscientiousness',\n",
        "    'BIG5_Neuroticism',\n",
        "    'BIG5_Openness'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c57b30b7-4b39-43c9-8d0c-5375e6ce0dc4",
      "metadata": {
        "id": "c57b30b7-4b39-43c9-8d0c-5375e6ce0dc4"
      },
      "source": [
        "## Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad078b3-e29d-4426-8960-68cd19bf55e8",
      "metadata": {
        "id": "8ad078b3-e29d-4426-8960-68cd19bf55e8"
      },
      "outputs": [],
      "source": [
        "# independent shaping personality test scores\n",
        "# 2250 profiles x 300 items = 675k rows\n",
        "test_scores = pd.read_pickle(ABL01_SCORES)\n",
        "\n",
        "# downstream task social media status updates\n",
        "# 2250 profiles x 25 repeats = 56,250 rows\n",
        "status_updates_raw = pd.read_pickle(STATUS_UPDATES_DATA)\n",
        "\n",
        "# AMS personality predictions based on status updates\n",
        "# 7 values per profile\n",
        "ams_predictions_raw = pd.read_csv(AMS_DATA, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25e9087-bb70-4ab6-99a5-13c2b7ad33a2",
      "metadata": {
        "id": "f25e9087-bb70-4ab6-99a5-13c2b7ad33a2"
      },
      "outputs": [],
      "source": [
        "if not IS_PALM_MODEL:\n",
        "  # pre-process status updates data\n",
        "  # consolidate every 25 updates under their prompted personality profile\n",
        "  status_updates = status_updates_raw.groupby('item_preamble_id')['model_output'].agg(list).reset_index()\n",
        "  status_updates['model_output'] = status_updates['model_output'].apply(lambda x: '\\n'.join(x))\n",
        "else:\n",
        "  status_updates = status_updates_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8ad455-199b-422d-b105-00d64fc1a132",
      "metadata": {
        "id": "8c8ad455-199b-422d-b105-00d64fc1a132"
      },
      "outputs": [],
      "source": [
        "status_updates.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb095631-7c50-475c-a701-c6cf13fb8f2e",
      "metadata": {
        "id": "cb095631-7c50-475c-a701-c6cf13fb8f2e"
      },
      "outputs": [],
      "source": [
        "# pre-process AMS data\n",
        "\n",
        "# create ID for every 7 rows\n",
        "ams_predictions_raw['ID'] = ams_predictions_raw.index // 7\n",
        "\n",
        "# pivot to wide\n",
        "ams_predictions_wide = ams_predictions_raw.pivot(\n",
        "    index=['ID', 'user_id'],\n",
        "    columns='trait',\n",
        "    values='value')\n",
        "\n",
        "# average AMS scores by shared prompt\n",
        "# 56,250 rows / 25 repetitions -\u003e 2,250 rows\n",
        "\n",
        "# group by `item_preamble_id` (labeled `user_id` here)\n",
        "ams_predictions = ams_predictions_wide.groupby('user_id').agg('mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9a1f4c-15c3-4fd4-a746-00345781e8ac",
      "metadata": {
        "id": "ea9a1f4c-15c3-4fd4-a746-00345781e8ac"
      },
      "outputs": [],
      "source": [
        "ams_predictions['user_id'] = ams_predictions.index\n",
        "ams_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492db886-e5c8-4f9b-bcae-d55714aca41a",
      "metadata": {
        "id": "492db886-e5c8-4f9b-bcae-d55714aca41a"
      },
      "source": [
        "## Join Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6860518-cd42-48f1-8397-ac625cec01cb",
      "metadata": {
        "id": "f6860518-cd42-48f1-8397-ac625cec01cb"
      },
      "outputs": [],
      "source": [
        "# attach ablation 01 scores to status updates\n",
        "# create partial IDs for matching\n",
        "status_updates['partial_id'] = status_updates['item_preamble_id'].str[:-4]\n",
        "test_scores['partial_id'] = test_scores['item_preamble_id'].str[:-4]\n",
        "ams_predictions['partial_id'] = ams_predictions['user_id'].str[:-4]\n",
        "\n",
        "# drop columns\n",
        "status_updates.drop('item_preamble_id', axis=1, inplace=True)\n",
        "test_scores.drop('item_preamble_id', axis=1, inplace=True)\n",
        "ams_predictions.drop('user_id', axis=1, inplace=True)\n",
        "\n",
        "if IS_PALM_MODEL:\n",
        "  df_grouped = pd.merge(pd.merge(test_scores, ams_predictions, on='partial_id'), status_updates, on='partial_id')\n",
        "else:\n",
        "  dfs = [status_updates, test_scores, ams_predictions]\n",
        "  dfs = [df.set_index('partial_id') for df in dfs]\n",
        "  df_grouped = dfs[0].join(dfs[1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad4dc27f-0360-49f8-bed7-b9f7b74b1c59",
      "metadata": {
        "id": "ad4dc27f-0360-49f8-bed7-b9f7b74b1c59"
      },
      "outputs": [],
      "source": [
        "# add intended personality level info\n",
        "LVL_IDS = ['lvl-EXT', 'lvl-AGR', 'lvl-CON', 'lvl-NEU', 'lvl-OPE']\n",
        "df_grouped['level_info'] = df_grouped['partial_id'] if IS_PALM_MODEL else df_grouped.index\n",
        "df_grouped[LVL_IDS + ['description_id']] = df_grouped['level_info'].str.split('-', expand=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e092df6-12fc-472a-b4b9-540981860e3f",
      "metadata": {
        "id": "6e092df6-12fc-472a-b4b9-540981860e3f"
      },
      "outputs": [],
      "source": [
        "def extract_integer(s):\n",
        "  \"\"\"Extract the level integer contained in a string.\"\"\"\n",
        "  result = ''\n",
        "  for char in s:\n",
        "    if char.isdigit():\n",
        "      result += char\n",
        "  try:\n",
        "    return int(result)\n",
        "  except ValueError:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17125d17-c2bc-4fa0-90fe-5131b7608048",
      "metadata": {
        "id": "17125d17-c2bc-4fa0-90fe-5131b7608048"
      },
      "outputs": [],
      "source": [
        "df_grouped[LVL_IDS + ['description_id']] = df_grouped[LVL_IDS + ['description_id']].map(extract_integer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb097320-a4bc-4b86-a0e8-a05b17b8864a",
      "metadata": {
        "id": "eb097320-a4bc-4b86-a0e8-a05b17b8864a"
      },
      "outputs": [],
      "source": [
        "# peek at new merged dataframe\n",
        "# should be 2,250 rows\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b6fb15-4ae2-4b54-8494-82d7e41c7d86",
      "metadata": {
        "id": "62b6fb15-4ae2-4b54-8494-82d7e41c7d86"
      },
      "outputs": [],
      "source": [
        "group = df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a850a3c-f8ed-4b86-a95a-37c6c1d24803",
      "metadata": {
        "id": "4a850a3c-f8ed-4b86-a95a-37c6c1d24803"
      },
      "outputs": [],
      "source": [
        "# optional: save scores to disk\n",
        "if SAVE_SCORES_FILENAME:\n",
        "  df_grouped.to_pickle(SAVE_SCORES_FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27bi1JDD1950",
      "metadata": {
        "id": "27bi1JDD1950"
      },
      "source": [
        "##Plot Wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bMFZXLj72AOH",
      "metadata": {
        "id": "bMFZXLj72AOH"
      },
      "outputs": [],
      "source": [
        "# Create a 2x5 layout for subplots\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 10))\n",
        "\n",
        "# Flatten the axes array\n",
        "axes = axes.flatten()\n",
        "\n",
        "stopwords = set()\n",
        "for w in wordcloud.STOPWORDS:\n",
        "  stopwords.add(w)\n",
        "\n",
        "for i, dimension in enumerate(['ext', 'agr', 'ope', 'con', 'neu']):\n",
        "  for j, level in enumerate([1, 9]):\n",
        "    # Concatenate all the values in the 'Text' column\n",
        "    text = ' '.join(group[group['partial_id'].str.contains(f'{dimension}{level}')]['model_output'].tolist())\n",
        "    # Create a WordCloud object\n",
        "    wc = wordcloud.WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text)\n",
        "    # Set the title for the subplot\n",
        "    axes[i*2 + j].set_title(f'{dimension}{level}')\n",
        "    # Plot the word cloud in the corresponding subplot\n",
        "    axes[i*2 + j].imshow(wc, interpolation='bilinear')\n",
        "    axes[i*2 + j].axis('off')\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Downstream_Task_Viz.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
