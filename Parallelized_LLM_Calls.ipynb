{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gEL53rg1G_j"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "This colab contains code used to run parallelized bulk inference on PaLM models for the paper \"Personality Traits in Large Language Models\" (https://arxiv.org/pdf/2307.00184). The code assumes that all the data produced and consumed in the colab lives in a local filesystem either in a cloud instance running a Jupyter notebook such as Google Colab or a desktop. But those file I/O operations can easily be replaced to use any other file management solutions.\n",
        "\n",
        "To run this colab:\n",
        "1. Connect to an appropriate runtime. (For instance, if running the bulk inference directly from the colab, connect to a GPU kernel.)\n",
        "2. Check experiment parameters below.\n",
        "3. Run the code cells for analysis.\n",
        "\n",
        "NOTE: Make sure to store and run this notebook from a location where the Psyborgs codebase package is stored (personality_in_llms.psyborgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj1ILwc2Wsc7"
      },
      "source": [
        "# **Experiment Parameters** - Set Before Running!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEmLtCrcf52A"
      },
      "source": [
        "**`batch_size`**: Number of prompts sent in one RPC if batch scoring is supported.\n",
        "\n",
        "**`num_workers`**: Number of threads (if running in parallel).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7bBq3O0IXHZW"
      },
      "outputs": [],
      "source": [
        "admin_session_json_path = 'prod_run_01_numeric_personachat_admin_session.json'  #@param ['BFI44_PANAS_numeric_demographic_admin_session'] {type:'string', allow-input: true, isTemplate: true}\n",
        "model_id = 'flan_palmchilla_62b_q'  #@param ['palm_62b_q', 'palmchilla_62b_q', 'flan_palmchilla_62b_q', 'flan_palm_540b_q', 'lamda-scoring', 'lamda-scoring-with-lm-scores']\n",
        "use_custom_model = True  #@param {type:'boolean'}\n",
        "batch_size = 4 #@param {type:'integer'}\n",
        "num_workers = 3 #@param {type:'integer'}\n",
        "results_filename = 'prod_run_01_flan_palmchilla_62b_q_scoring_numeric.pkl'  #@param {type:'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyXlfbOfvGSf"
      },
      "source": [
        "Modify this if `use_custom_model` is True:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FGKmdPpmu0eW"
      },
      "outputs": [],
      "source": [
        "user_readable_name = 'Flan-PaLMChilla 62B (Quantized)'  #@param ['PaLM 62B (quantized)', 'PaLMChilla 62B (quantized)', 'FLAN-PaLMChilla 62B (quantized)', 'FLAN-PaLM 540B (quantized)', 'Base Meena2-64B dialog gen 41 for scoring', 'Base Meena2-64B dialog gen 41 for scoring with response score enabled'] {type:'string', allow-input: true, isTemplate: true}\n",
        "model_endpoint = '/some/endpoint/flan_palmchilla_62b_q'  #@param ['/some/endpoint/palmchilla_62b_q', '/some/endpoint/flan_palmchilla_62b_q'] {type:'string', allow-input: true, isTemplate: true}\n",
        "model_family = 'PaLM' #@param ['PaLM', 'LaMDA'] {type:'string'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKbz65tlrj-l"
      },
      "source": [
        "# Load Libraries \u0026 Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7h-4OU5ZJZd"
      },
      "source": [
        "### Install Psyborgs Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ5zCEVAGeMo"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this cell to install the dependencies needed to run Psyborgs.\n",
        "#@markdown The dependencies are in a requirements.txt file in the Psyborgs repo.\n",
        "%pip install -r psyborgs/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqaKGoVRUof"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT7WvoUCrWPL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import dataclasses\n",
        "import enum\n",
        "import functools\n",
        "from typing import List, Dict, Tuple, Iterator, Optional, Union\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from psyborgs import llm_scoring_lib, survey_bench_lib, parallel\n",
        "\n",
        "import dacite\n",
        "import pandas as pd\n",
        "import io\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njawJSNgbRwD"
      },
      "outputs": [],
      "source": [
        "# this block is used to change the model specification\n",
        "if model_family == 'PaLM':\n",
        "  model_family = survey_bench_lib.ModelFamily.PALM\n",
        "elif model_family == 'LaMDA':\n",
        "  model_family = survey_bench_lib.ModelFamily.LAMDA\n",
        "\n",
        "if use_custom_model:\n",
        "  model_spec = survey_bench_lib.ModelSpec(\n",
        "      user_readable_name=user_readable_name,\n",
        "      model_family=model_family,\n",
        "      model_endpoint=model_endpoint,\n",
        "  )\n",
        "else:\n",
        "  model_spec = model_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyWY8-oNZOgQ"
      },
      "source": [
        "# Load AdministrationSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBGtSUIgZQQp"
      },
      "outputs": [],
      "source": [
        "def load_admin_session(json_path: str):\n",
        "  json_file = drive.LoadFile(json_path)\n",
        "  admin_session_dict = json.loads(json_file)\n",
        "\n",
        "  # dacite documentation on casting input values to objects can be found here:\n",
        "  # https://github.com/konradhalas/dacite#casting\n",
        "  admin_session = dacite.from_dict(data_class=survey_bench_lib.AdministrationSession,\n",
        "                                   data=admin_session_dict,\n",
        "                                   config=dacite.Config(cast=[enum.Enum]))\n",
        "\n",
        "  return admin_session\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vasrjouIiUY"
      },
      "source": [
        "# `administer_session` Refactored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWjNyU12Iw7m"
      },
      "outputs": [],
      "source": [
        "def administer_session_serially(admin_session: survey_bench_lib.AdministrationSession,\n",
        "                                verbose: bool = False) -\u003e pd.DataFrame:\n",
        "  \"\"\"Administers specified survey items to LLMs and returns raw LLM scores.\n",
        "\n",
        "  This key function (serial version) 'administers' a battery of survey measures\n",
        "    to various LLMs specified within an `AdministrationSession` object. Since\n",
        "    items (e.g., 'I like ice cream') within a measure can be presented to LLMs\n",
        "    in a variety of ways, each item is administered multiple times across an\n",
        "    assortment of compatible framing options and standardized response choices\n",
        "    derived from response scales.\n",
        "\n",
        "    Framing options within an `AdministrationSession` consist of item preambles\n",
        "    (e.g., 'With regards to the following statement, \"'), item postambles\n",
        "    (e.g., '\", I tend to '), and response choice postambles (e.g., '.').\n",
        "\n",
        "    Prompts and continuations are assembled in the following format:\n",
        "\n",
        "    Prompt:\n",
        "    {item preamble} {item} {item postamble}\n",
        "\n",
        "    Continuation:\n",
        "    {response choice} {response choice postamble}\n",
        "\n",
        "  Args:\n",
        "    admin_session: An `AdministrationSession` containing a specification of\n",
        "      desired survey measures, item framing options, and LLM scoring functions.\n",
        "    verbose: If True, output is printed for debugging.\n",
        "\n",
        "  Returns:\n",
        "    A Pandas DataFrame containing raw LLM scores for each item-response choice\n",
        "      pair and specification information needed to reproduce the score.\n",
        "  \"\"\"\n",
        "  # create dict of LLM scoring functions for reuse\n",
        "  llm_scoring_fns = survey_bench_lib.create_llm_scoring_fns_dict(admin_session)\n",
        "\n",
        "  # for efficiency, accumulate raw score data for each item + response choice +\n",
        "  # options combination in a list, then this list to a pd.DataFrame at the end\n",
        "  # of the loop\n",
        "  raw_response_scores_list = []\n",
        "\n",
        "  # iterate through all measures and scale combinations\n",
        "  for measure_object in survey_bench_lib.measure_generator(admin_session):\n",
        "\n",
        "    # iterate through all prompt combinations\n",
        "    for prompt_object in survey_bench_lib.prompt_generator(\n",
        "        measure_object, admin_session):\n",
        "\n",
        "      # iterate through all continuation combinations\n",
        "      for continuation_object in survey_bench_lib.continuation_generator(\n",
        "          measure_object, admin_session):\n",
        "\n",
        "        # iterate through LLM scoring functions to use (this is done here to\n",
        "        # preempt potential RPC rate limits)\n",
        "        for model_id, model_scoring_fn in llm_scoring_fns.items():\n",
        "\n",
        "          # assemble and score payload\n",
        "          raw_score = survey_bench_lib.assemble_and_score_payload(\n",
        "              measure_object, prompt_object, continuation_object,\n",
        "              model_scoring_fn, model_id, verbose)\n",
        "\n",
        "          # append single score + specification info\n",
        "          raw_response_scores_list.append(raw_score)\n",
        "\n",
        "  # convert raw scores list into pd.DataFrame\n",
        "  raw_response_scores_df = pd.DataFrame(raw_response_scores_list)\n",
        "\n",
        "  return raw_response_scores_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q--fnDNVaCH8"
      },
      "outputs": [],
      "source": [
        "def administer_session_serially_by_model(\n",
        "    admin_session: survey_bench_lib.AdministrationSession,\n",
        "    model_spec: Union[str, survey_bench_lib.ModelSpec],\n",
        "    verbose: bool = False) -\u003e pd.DataFrame:\n",
        "  \"\"\"Administers session serially for one model spec.\"\"\"\n",
        "  # create dict of LLM scoring functions for reuse\n",
        "  model_scoring_fn = survey_bench_lib.create_llm_scoring_fn(model_spec)\n",
        "\n",
        "  # for efficiency, accumulate raw score data for each item + response choice +\n",
        "  # options combination in a list, then this list to a pd.DataFrame at the end\n",
        "  # of the loop\n",
        "  raw_response_scores_list = []\n",
        "\n",
        "  # iterate through all measures and scale combinations\n",
        "  for measure_object in survey_bench_lib.measure_generator(admin_session):\n",
        "\n",
        "    # iterate through all prompt combinations\n",
        "    for prompt_object in survey_bench_lib.prompt_generator(\n",
        "        measure_object, admin_session):\n",
        "\n",
        "      # iterate through all continuation combinations\n",
        "      for continuation_object in survey_bench_lib.continuation_generator(\n",
        "          measure_object, admin_session):\n",
        "\n",
        "        # assemble and score payload\n",
        "        raw_score = survey_bench_lib.assemble_and_score_payload(\n",
        "            measure_object, prompt_object, continuation_object,\n",
        "            model_scoring_fn, model_id, verbose)\n",
        "\n",
        "        # append single score + specification info\n",
        "        raw_response_scores_list.append(raw_score)\n",
        "\n",
        "  # convert raw scores list into pd.DataFrame\n",
        "  raw_response_scores_df = pd.DataFrame(raw_response_scores_list)\n",
        "\n",
        "  return raw_response_scores_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Is1pmQD_dY"
      },
      "source": [
        "# administer_session Parallel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5EesnIvcRZv"
      },
      "outputs": [],
      "source": [
        "def generate_payload_df(admin_session: survey_bench_lib.AdministrationSession,\n",
        "                        model_id: str) -\u003e pd.DataFrame:\n",
        "  \"\"\"Returns sorted df of prompts, continuations, and info to be scored.\"\"\"\n",
        "  # accumulate payloads in a list to be sent to LLM endpoints in parallel\n",
        "  payload_list = []\n",
        "\n",
        "  # iterate through all measures and scale combinations\n",
        "  for measure_iteration in survey_bench_lib.measure_generator(admin_session):\n",
        "\n",
        "    # iterate through all prompt combinations\n",
        "    for prompt_iteration in survey_bench_lib.prompt_generator(\n",
        "        measure_iteration, admin_session):\n",
        "\n",
        "      # iterate through all continuation combinations\n",
        "      for continuation_iteration in survey_bench_lib.continuation_generator(\n",
        "          measure_iteration, admin_session):\n",
        "\n",
        "        # generate payload spec with null scores and set model_id\n",
        "        payload_spec = survey_bench_lib.generate_payload_spec(\n",
        "            measure_iteration, prompt_iteration, continuation_iteration, 0,\n",
        "            model_id)\n",
        "        payload_list.append(payload_spec)\n",
        "\n",
        "  # dataframe is sorted by prompt, continuation\n",
        "  return pd.DataFrame(payload_list).sort_values(\n",
        "      ['prompt_text', 'continuation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zFiAN5wHFXR"
      },
      "source": [
        "## Parallel: 1 prompt, 1 continuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWINYae_NGY"
      },
      "source": [
        "## Batch Parallel: 1 prompt, multiple continuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc5xWapbAh-M"
      },
      "outputs": [],
      "source": [
        "DATASET = [(i%4, i) for i in range(32)]  # list of (prompt, continuation)\n",
        "\n",
        "def process_batch(p, c_list) -\u003e float: # list\n",
        "  return [p + c / 10. for c in c_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mr8_aaWlXI2"
      },
      "outputs": [],
      "source": [
        "def chunk_array(array, chunk_size):\n",
        "    return np.split(array, np.arange(chunk_size, len(array), chunk_size))\n",
        "\n",
        "\n",
        "def build_arg_for_parallel_call(df_, batch_size):\n",
        "  arg_for_parallel_call = []\n",
        "  for prompt, sub_df in df_.groupby(['prompt_text']):\n",
        "    for chunk in chunk_array(sub_df['continuation_text'], batch_size):\n",
        "      arg_for_parallel_call.append({\n",
        "          'prompt': prompt,\n",
        "          'continuations': chunk.values.tolist(),\n",
        "      })\n",
        "  return arg_for_parallel_call\n",
        "\n",
        "\n",
        "def compute_and_flatten_result(df_, llm_batched_scoring_fn, num_workers,\n",
        "                               batch_size):\n",
        "  batched_results = parallel.RunInParallel(\n",
        "        llm_batched_scoring_fn,\n",
        "        build_arg_for_parallel_call(df_, batch_size),\n",
        "        num_workers=num_workers,\n",
        "        report_progress=True)\n",
        "  return list(itertools.chain(*batched_results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbqwjWe4VZPY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def score_batches_in_parallel(\n",
        "    payloads: List[tuple],\n",
        "    llm_batched_scoring_fn: llm_scoring_lib.LanguageModelScoringFn,\n",
        "    batch_size: int = 4,\n",
        "    num_workers: int = 8) -\u003e List[float]:\n",
        "  \"\"\"Scores a payload in batches of continuations grouped by prompt.\n",
        "\n",
        "  Args:\n",
        "    payloads: An unordered list of (prompt_text, continuation_text) tuple\n",
        "      payloads to be scored.\n",
        "    llm_batched_scoring_fn: An LLM scoring function capable of batch scoring of\n",
        "      continuations (i.e., processing 1 prompt and 1 or multiple continuations).\n",
        "      Should return a sequence of float values, one for each passed\n",
        "      continuation.\n",
        "    num_workers: Number of threads in parallel for making RPCs to LLM services.\n",
        "\n",
        "  Returns:\n",
        "    An list of float scores ordered by prompt and continuation of the `payloads`\n",
        "      input.\n",
        "  \"\"\"\n",
        "\n",
        "  def _score_multiple_continuations(prompt, continuations):\n",
        "    try:\n",
        "      return llm_batched_scoring_fn(prompt, continuations)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      # print(f'RPC exception')\n",
        "      time.sleep(1)\n",
        "      return _score_multiple_continuations(prompt, continuations)\n",
        "\n",
        "  # create dataframe from tuples input\n",
        "  df = pd.DataFrame(\n",
        "      payloads,\n",
        "      columns=['prompt_text', 'continuation_text']\n",
        "  )\n",
        "\n",
        "  result = compute_and_flatten_result(\n",
        "      df, _score_multiple_continuations, num_workers=num_workers,\n",
        "      batch_size=batch_size)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE4WF5GXd45y"
      },
      "outputs": [],
      "source": [
        "def process_batch_str(p, c_list) -\u003e str: # list\n",
        "  return [p + c for c in c_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e87f-AwRamd3"
      },
      "source": [
        "# Call in Parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY7FPd4AhgXy"
      },
      "source": [
        "## Administer Session in Batched Parallel Calls by Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBDE6G0ahnKz"
      },
      "outputs": [],
      "source": [
        "def administer_session_in_batched_parallel_calls_by_model(\n",
        "    payload_df: pd.DataFrame,\n",
        "    # model_id: str,\n",
        "    model_spec: Union[str, survey_bench_lib.ModelSpec],\n",
        "    batch_size: int = 4,\n",
        "    num_workers: int = 8,\n",
        "    debug: bool = False) -\u003e pd.DataFrame:\n",
        "  \"\"\"Administers surveys through parallel calls batched by unique prompts.\n",
        "\n",
        "  Args:\n",
        "    admin_session_json_path: Relative path of JSON file containing the\n",
        "      specification of an `AdministrationSession`.\n",
        "    model_id: String ID of model to be queried. Used to point to the correct\n",
        "      model endpoint/blade target. Example: 'palmchilla_62b_q'.\n",
        "    num_workers: Number of threads in parallel for making RPCs to LLM services.\n",
        "    debug: If True, this function replaces LLM RPCs with calls to a dummny\n",
        "      scoring function for prompt, continuation string inputs (i.e.,\n",
        "      `process_batch_str()`).\n",
        "\n",
        "  Returns:\n",
        "    A Pandas DataFrame with columns `prompt_text`, `continuation_text`, `score`,\n",
        "      and all other payload specification information.\n",
        "  \"\"\"\n",
        "\n",
        "  # define helper function(s)\n",
        "  def _to_scorable_tuple_list(payload_df: pd.DataFrame) -\u003e List[tuple]:\n",
        "    \"\"\"Converts prompt, continuation cols of `payload_df` to a list of tuples.\"\"\"\n",
        "    return list(zip(payload_df['prompt_text'], payload_df['continuation_text']))\n",
        "\n",
        "  def _process_batch_str(p, c_list) -\u003e str:  # list\n",
        "    return [p + c for c in c_list]\n",
        "\n",
        "  # get scorable list of tuples\n",
        "  payload_list = _to_scorable_tuple_list(payload_df)\n",
        "\n",
        "  # score by batches of continuations in parallel\n",
        "  # if debugging, use `process_batch_str()` as scoring fn instead\n",
        "  if debug:\n",
        "    results_list = score_batches_in_parallel(\n",
        "        payload_list, _process_batch_str, num_workers, batch_size=batch_size)\n",
        "\n",
        "  # otherwise, use a real LLM scoring function\n",
        "  else:\n",
        "    # create scoring function based on specified model_id\n",
        "    # if debugging, do not use a real LLM scoring function\n",
        "    if type(model_spec) == str:\n",
        "      llm_model_spec = admin_session.models[model_id]\n",
        "    elif type(model_spec) == survey_bench_lib.ModelSpec:\n",
        "      llm_model_spec = model_spec\n",
        "    else:\n",
        "      raise ValueError(f'model_spec is not a recognized type!')\n",
        "\n",
        "    llm_batched_scoring_fn = survey_bench_lib.create_llm_scoring_fn(\n",
        "        llm_model_spec)\n",
        "\n",
        "    results_list = score_batches_in_parallel(\n",
        "        payloads=payload_list,\n",
        "        llm_batched_scoring_fn=llm_batched_scoring_fn,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers)\n",
        "\n",
        "  # append results list as new column to payload_df\n",
        "  payload_df['score'] = results_list\n",
        "\n",
        "  return payload_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-usw4aavcpO_"
      },
      "source": [
        "# Main Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc4YOjDR3keu"
      },
      "outputs": [],
      "source": [
        "# load admin session from json\n",
        "admin_session = load_admin_session(admin_session_json_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSF4cFhu3qVf"
      },
      "outputs": [],
      "source": [
        "# generate payload_spec_df\n",
        "payload_df = generate_payload_df(admin_session, model_id)\n",
        "payload_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRgBZ5lvdCb0"
      },
      "outputs": [],
      "source": [
        "# if using a custom model specification, set model_spec is set at the top of\n",
        "# this notebook.\n",
        "session_scores = administer_session_in_batched_parallel_calls_by_model(\n",
        "    payload_df=payload_df,\n",
        "    model_spec=model_spec,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EUdr6IJB7xE"
      },
      "outputs": [],
      "source": [
        "session_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4cLF8MlEpvO"
      },
      "outputs": [],
      "source": [
        "#@title Convert to .pkl and output\n",
        "#@markdown Run this cell to convert dataframe into pickle and dump to location\n",
        "with open(results_filename, 'wb') as f:\n",
        "    pickle.dump(session_scores, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1Lk6XUpMzfkI5_DjQuvvMoxK_mby83OJE",
          "timestamp": 1672912311025
        },
        {
          "file_id": "1geRlmOdI-CC_A5ocX8RzB957d0PF5Udx",
          "timestamp": 1672819768348
        },
        {
          "file_id": "1Y-JVNZpx2cyinzHzYh_4UzTd1y8KnJXh",
          "timestamp": 1671230660488
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
