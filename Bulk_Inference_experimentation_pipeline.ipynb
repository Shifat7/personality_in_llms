{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXj5nuUHPcwv"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "This colab contains code used to run bulk inference on PaLM models for the paper \"Personality Traits in Large Language Models\" (https://arxiv.org/pdf/2307.00184). The code assumes that all the data produced and consumed in the colab lives in a local filesystem either in a cloud instance running a Jupyter notebook such as Google Colab or a desktop. But those file I/O operations can easily be replaced to use any other file management solutions.\n",
        "\n",
        "The colab is composed of 3 steps:\n",
        "* Step 1: Read in the input data structure for the experiment - Admin Session (from Drive) and convert it into TFRecord format and store it back in the filesystem (so that this step can be skipped next time). Admin sessions are defined here: https://github.com/google-research/google-research/blob/master/psyborgs/survey_bench_lib.py#L70 and are part of the PsyBORGS open source framework (https://github.com/google-research/google-research/tree/master/psyborgs). This forms the input to the bulk LLM inference script (Step 2).\n",
        "* Step 2: Run the bulk LLM inference script on (separately started) prediction servers using PaLM. The script needs to be configured with the name of the input TFRecord (output of Step 1), and name of the output TFRecord (input to Step 3).\n",
        "* Step 3: For PaLM models, the output is produced in a TFRecord format. Read in the TFRecord output from the bulk inference script, and convert it into pickle format to store it back in the filesystem in use. It needs to be in .pkl format to be used by the Personality Analysis pipeline.\n",
        "\n",
        "To run this colab:\n",
        "1. Connect to an appropriate runtime. (For instance, if running the bulk inference directly from the colab, connect to a GPU kernel.)\n",
        "2. Check experiment parameters below.\n",
        "3. Run Step 1 from above.\n",
        "4. Ensure the TFRecord file exists in the intended location.\n",
        "5. Update bulk inference script with filenames for input and output, and run.\n",
        "6. Once the bulk inference completes, do a consistency check on the TFRecord output.\n",
        "7. Run Step 3.\n",
        "\n",
        "NOTE: Make sure to store and run this notebook from a location where the Psyborgs codebase package is stored (personality_in_llms.psyborgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjWFPGlVxrm-"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1IEJ-wp7skz"
      },
      "source": [
        "The repo containing this notebook has a version of the Psyborgs codebase needed to make the notebook run. But in case a more recent version is needed, it can be fetched from https://github.com/google-research/google-research/tree/master/psyborgs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgku2yNKt2sU",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@markdown Run this cell to install the dependencies needed to run Psyborgs.\n",
        "#@markdown The dependencies are in a requirements.txt file in the Psyborgs repo.\n",
        "%pip install -r psyborgs/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQaCEWMLxcSX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@title Load Libraries\n",
        "#@markdown Run this cell to import dependencies\n",
        "import enum\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from psyborgs import survey_bench_lib\n",
        "import dacite\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhS51wJsPrs3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Experiment Parameters  { run: \"auto\" }\n",
        "#@markdown Run this cell to setup the file locations.\n",
        "\n",
        "#@markdown `admin_session_file_path` is the file path of the input admin session that needs to be bulk inferred on.\n",
        "admin_session_file_path = 'admin_sessions/sample_admin_session.json'  # @param {\"type\":\"string\"}\n",
        "#@markdown `tfrecord_file_path` is the file path of the TFRecord file that gets dumped after converting it from the admin session.\n",
        "#@markdown It is the admin session unrolled into individual inference prompts that will be run on\n",
        "#@markdown the bulk LLM inference pipeline. So this TFRecord is the input to the bulk LLM inference pipeline.\n",
        "tfrecord_file_path = 'sample_file.tfrecord'  #@param {type:'string'}\n",
        "#@markdown `llm_output_tfrecord_file_path` is the file path where the output of the LLM bulk inference is stored.\n",
        "llm_output_tfrecord_file_path = 'sample_llm_output.tfrecord'  #@param {type:\"string\"}\n",
        "#@markdown `output_pkl_filepath` is the path to the pkl file that contains the LLM inference output dataframe.\n",
        "output_pkl_filepath = 'sample_llm_output.pkl'  #@param {type:\"string\"}\n",
        "#@markdown ####Below are settings relevant to multi-sharded runs\n",
        "#@markdown `max_num_rows_per_shard` is the maximum number of payload specs per shard of the final tfrecord created from the admin session.\n",
        "max_num_rows_per_shard = 4800000  #@param {type:\"integer\"}\n",
        "num_shards = 1  #@param {type:\"number\"}\n",
        "#@markdown this is a model identifier needed for Psyborgs code. More info here: psyborgs/survey_bench_lib.py:L63\n",
        "model_id = 'PaLM'  #@param {type:\"string\"}\n",
        "use_custom_model = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKc6FlgnV31U"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrAFqK3OPvOo"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this cell to setup the util functions needed for this colab\n",
        "\n",
        "def format_shard(shard_idx: int):\n",
        "  return str(shard_idx).zfill(5)\n",
        "\n",
        "\n",
        "def load_admin_session(admin_session_filename: str):\n",
        "  with open(admin_session_filename, 'r') as admin_session_file:\n",
        "    admin_session_dict = json.load(admin_session_file)\n",
        "\n",
        "  # dacite documentation on casting input values to objects can be found here:\n",
        "  # https://github.com/konradhalas/dacite#casting\n",
        "  session = dacite.from_dict(data_class=survey_bench_lib.AdministrationSession,\n",
        "                             data=admin_session_dict,\n",
        "                             config=dacite.Config(cast=[enum.Enum]))\n",
        "\n",
        "  return session\n",
        "\n",
        "def generate_payload_df(input_admin_session: survey_bench_lib.AdministrationSession,\n",
        "                        input_model_id: str) -> pd.DataFrame:\n",
        "  \"\"\"Returns sorted df of prompts, continuations, and info to be scored.\"\"\"\n",
        "  # accumulate payloads in a list to be sent to LLM endpoints in parallel\n",
        "  payload_list = []\n",
        "\n",
        "  # iterate through all measures and scale combinations\n",
        "  for measure_iteration in survey_bench_lib.measure_generator(admin_session):\n",
        "\n",
        "    # iterate through all prompt combinations\n",
        "    for prompt_iteration in survey_bench_lib.prompt_generator(\n",
        "        measure_iteration, input_admin_session):\n",
        "\n",
        "      # iterate through all continuation combinations\n",
        "      for continuation_iteration in survey_bench_lib.continuation_generator(\n",
        "          measure_iteration, input_admin_session):\n",
        "\n",
        "        # generate payload spec with null scores and set model_id\n",
        "        payload_spec = survey_bench_lib.generate_payload_spec(\n",
        "            measure_iteration, prompt_iteration, continuation_iteration, 0,\n",
        "            input_model_id)\n",
        "        payload_list.append(payload_spec)\n",
        "\n",
        "  # dataframe is sorted by prompt, continuation\n",
        "  return pd.DataFrame(payload_list).sort_values(\n",
        "      ['prompt_text', 'continuation_text'])\n",
        "\n",
        "def generate_payload_row(input_admin_session: survey_bench_lib.AdministrationSession,\n",
        "                         input_model_id: str) -> survey_bench_lib.PayloadSpec:\n",
        "  \"\"\"Returns sorted df of prompts, continuations, and info to be scored.\"\"\"\n",
        "  # iterate through all measures and scale combinations\n",
        "  for measure_iteration in survey_bench_lib.measure_generator(admin_session):\n",
        "\n",
        "    # iterate through all prompt combinations\n",
        "    for prompt_iteration in survey_bench_lib.prompt_generator(\n",
        "        measure_iteration, input_admin_session):\n",
        "\n",
        "      # iterate through all continuation combinations\n",
        "      for continuation_iteration in survey_bench_lib.continuation_generator(\n",
        "          measure_iteration, input_admin_session):\n",
        "\n",
        "        # generate payload spec with null scores and set model_id\n",
        "        yield survey_bench_lib.generate_payload_spec(\n",
        "            measure_iteration, prompt_iteration, continuation_iteration, 0,\n",
        "            input_model_id)\n",
        "\n",
        "\n",
        "def write_df_as_tfrecord(input_payload_df: pd.DataFrame, shard_idx: int = 0):\n",
        "  \"\"\"Writes the input dataframe as a TFRecord file.\"\"\"\n",
        "  # Define the TFRecord filename\n",
        "  tfrecord_filename = f'{tfrecord_file_path}_{format_shard(shard_idx)}'\n",
        "\n",
        "  # Create a TFRecord writer\n",
        "  with tf.io.TFRecordWriter(tfrecord_filename) as w:\n",
        "    # Loop over the dataframe and serialize each row\n",
        "    for r in input_payload_df.itertuples(index=False):\n",
        "      # Create a feature dictionary from the row data\n",
        "      feature_map = {}\n",
        "      for col, val in zip(input_payload_df.columns, r):\n",
        "        if input_payload_df[col].dtype == 'int64':\n",
        "          feature_map[col] = tf.train.Feature(\n",
        "              int64_list=tf.train.Int64List(value=[val])\n",
        "          )\n",
        "        elif input_payload_df[col].dtype == 'float64':\n",
        "          feature_map[col] = tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(value=[val])\n",
        "          )\n",
        "        else:\n",
        "          feature_map[col] = tf.train.Feature(\n",
        "              bytes_list=tf.train.BytesList(value=[val.strip().encode('utf-8')])\n",
        "          )\n",
        "      ex = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
        "      # Serialize the example\n",
        "      serial_example = ex.SerializeToString()\n",
        "      # Write the serialized example to the TFRecord file\n",
        "      w.write(serial_example)\n",
        "\n",
        "\n",
        "def write_payload_df(input_admin_session: survey_bench_lib.AdministrationSession,\n",
        "                     input_model_id: str) -> int:\n",
        "  \"\"\"Returns sorted df of prompts, continuations, and info to be scored.\"\"\"\n",
        "  # accumulate payloads in a list to be sent to LLM endpoints in parallel\n",
        "\n",
        "  # iterate through all measures and scale combinations\n",
        "  shards = 0\n",
        "  while True:\n",
        "    payload_list = []\n",
        "    num_rows = 0\n",
        "    for payload_spec in generate_payload_row(input_admin_session, input_model_id):\n",
        "      payload_list.append(payload_spec)\n",
        "      num_rows += 1\n",
        "      if num_rows >= max_num_rows_per_shard: break\n",
        "    if not payload_list: break\n",
        "    # dataframe is sorted by prompt, continuation\n",
        "    input_payload_df = pd.DataFrame(payload_list).sort_values(\n",
        "        ['prompt_text', 'continuation_text'])\n",
        "    write_df_as_tfrecord(input_payload_df, shards)\n",
        "    shards += 1\n",
        "  return shards\n",
        "\n",
        "# Define a parsing function to extract the features\n",
        "def parse_example(ex):\n",
        "  return tf.io.parse_single_example(ex, feature_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UMYPdajbD0x4"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this cell to setup the feature column names, so that the inferred examples in the TFRecords files can be correctly extracted and translated into a dataframe.\n",
        "# Define a default feature description dictionary\n",
        "feature_description = {\n",
        "    'continuation_text': tf.io.FixedLenFeature([], tf.string),\n",
        "    'item_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'item_postamble_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'item_preamble_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'measure_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'measure_name': tf.io.FixedLenFeature([], tf.string),\n",
        "    'model_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    # 'model_output': tf.io.FixedLenFeature([], tf.string),\n",
        "    'model_output_score': tf.io.FixedLenFeature([], tf.float32),\n",
        "    'prompt_text': tf.io.FixedLenFeature([], tf.string),\n",
        "    'response_choice': tf.io.FixedLenFeature([], tf.string),\n",
        "    'response_choice_postamble_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'response_scale_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'response_value': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'scale_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'score': tf.io.FixedLenFeature([], tf.int64),\n",
        "}\n",
        "\n",
        "# Define the relevant feature names for deduplication\n",
        "dedup_feature_names = [\n",
        "    'item_preamble_id',\n",
        "    'item_postamble_id',\n",
        "    'response_scale_id',\n",
        "    'response_choice_postamble_id',\n",
        "    'model_id']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZNEj8r0P3CV"
      },
      "source": [
        "# Step 1) Admin Session -> TFRecord\n",
        "This step also adds some parameters needed for the LLM inference pipelines to directly ingest and work with the input files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ps5tbciP2om"
      },
      "outputs": [],
      "source": [
        "# @title Convert admin session to dataframe {\"run\":\"auto\"}\n",
        "#@markdown Load admin session from json and generate payload_spec dataframe.\n",
        "#@markdown This dataframe is used for rest of the code below.\n",
        "\n",
        "admin_session = load_admin_session(admin_session_file_path)\n",
        "payload_df = generate_payload_df(admin_session, model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P76ZfmHOJjNT"
      },
      "source": [
        "### [For PaLM models only] Convert Dataframe to TFRecord and write to filesystem\n",
        "Define the feature description dictionary and write TFRecord file to notebook-local location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nGtQZH-QAK3"
      },
      "outputs": [],
      "source": [
        "# Create a TFRecord writer\n",
        "with tf.io.TFRecordWriter(tfrecord_file_path) as writer:\n",
        "  # Loop over the dataframe and serialize each row\n",
        "  for row in payload_df.itertuples(index=False):\n",
        "    # Create a feature dictionary from the row data\n",
        "    feature_dict = {}\n",
        "    for column, value in zip(payload_df.columns, row):\n",
        "      if payload_df[column].dtype == 'int64':\n",
        "        feature_dict[column] = tf.train.Feature(\n",
        "            int64_list=tf.train.Int64List(value=[value])\n",
        "        )\n",
        "      elif payload_df[column].dtype == 'float64':\n",
        "        feature_dict[column] = tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(value=[value])\n",
        "        )\n",
        "      else:\n",
        "        feature_dict[column] = tf.train.Feature(\n",
        "            bytes_list=tf.train.BytesList(value=[value.strip().encode('utf-8')])\n",
        "        )\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
        "    # Serialize the example\n",
        "    serialized_example = example.SerializeToString()\n",
        "    # Write the serialized example to the TFRecord file\n",
        "    writer.write(serialized_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOKM7J_xUT4L"
      },
      "source": [
        "# Step 2) Run Bulk Inference Script\n",
        "\n",
        "Depending on whichever model is chosen, this step needs to be done outside this colab by executing the bulk inference script from CLI against your model of choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHmonZc5EsHU"
      },
      "source": [
        "# Step 3) [PaLM models] TFRecord -> Pickle file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsuRqZFgVNbi"
      },
      "source": [
        "## Read Input TFRecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr0ssEAXxqRe"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this cell to read the TFRecords sharded files as a dataset\n",
        "dataset = tf.data.Dataset.list_files(llm_output_tfrecord_file_path, shuffle=False)\n",
        "dataset = dataset.flat_map(tf.data.TFRecordDataset)\n",
        "\n",
        "# Parse the dataset using the parsing function\n",
        "parsed_dataset = dataset.map(parse_example)\n",
        "\n",
        "# Convert the parsed dataset to a list of dictionaries\n",
        "list_of_dicts = []\n",
        "for example in parsed_dataset:\n",
        "  example_dict = {}\n",
        "  for key in example.keys():\n",
        "    example_dict[key] = example[key].numpy()\n",
        "  list_of_dicts.append(example_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysIeH_nvEWR8"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this cell to convert the dataset to a Pandas DataFrame\n",
        "df = pd.DataFrame(list_of_dicts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz415BBLI4tx"
      },
      "source": [
        "## Scoring mode model output column creation\n",
        "Run this cell only if running scoring mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6xTEXtc6HzZ"
      },
      "outputs": [],
      "source": [
        "df['model_output'] = df['continuation_text'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8fCb3YitO2k"
      },
      "source": [
        "## Generative text processing\n",
        "Run the cells below only if running experiments for generating text e.g. for downstream tasks and not for LLM survey responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s-nw4eQjXIG"
      },
      "outputs": [],
      "source": [
        "#@markdown Remove columns that are unnecessary.\n",
        "string_cols = list(feature_description.keys())\n",
        "string_cols.remove('score')\n",
        "string_cols.remove('model_output_score')\n",
        "string_cols.remove('response_value')\n",
        "df[string_cols] = df[string_cols].applymap(lambda x: x.decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSNVfjBvhaaQ"
      },
      "outputs": [],
      "source": [
        "#@markdown Make sure the needed columns have the required format.\n",
        "groupings = {k: 'first' for k in feature_description.keys()}\n",
        "groupings['model_output'] = lambda x: '<SEP> '.join(x)\n",
        "for dedup_feature_name in dedup_feature_names:\n",
        "  del groupings[dedup_feature_name]\n",
        "grouped_df = df.groupby(dedup_feature_names).agg(groupings).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy5BwxPXk9Y-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@markdown [Optional] Plot histogram of string lengths.\n",
        "\n",
        "grouped_df['model_output_len'] = grouped_df['model_output'].apply(lambda x: len(x.split()))\n",
        "plt.hist(grouped_df['model_output_len'], bins='auto', edgecolor='black')\n",
        "plt.xlabel('String Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of String Lengths')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtlnoa_R_zRK"
      },
      "outputs": [],
      "source": [
        "#@markdown Prep dataframe to be written.\n",
        "df = grouped_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92tOKA4oVoJw"
      },
      "source": [
        "## Convert to .pkl and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tum8NlGF6Hza"
      },
      "outputs": [],
      "source": [
        "#@title Convert to .pkl and output\n",
        "#@markdown Run this cell to convert dataframe into pickle and dump to location\n",
        "with open(output_pkl_filepath, 'wb') as f:\n",
        "  pickle.dump(df, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1dUPErAtRXxsIjja06BYNz7z8MhhZlxgQ",
          "timestamp": 1692999747630
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
