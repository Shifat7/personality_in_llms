{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7781c8",
   "metadata": {},
   "source": [
    "# Setup\n",
    "This template analyzes the effectiveness of shaping **one** synthetic personality trait of a model at a time (i.e., without shaping instructions for any other traits). Use `multidimensional_trait_shaping_analysis.ipynb` to see how well a given model can simulate control of multiple traits at once.\n",
    "\n",
    "1. Specify your model's full results pickle file, JSON `admin_session`, and identifier (model pointer), below.\n",
    "2. If you'd like to save the test scores for further analysis, specify a `SAVE_SCORES_FILENAME`.\n",
    "3. Run this notebook in `personality_in_llms/analysis` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b886857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory containing psyborgs\n",
    "# this default path should work if you've cloned the repo\n",
    "PATH = \"../\" \n",
    "\n",
    "# psychometric utils path (no need to change)\n",
    "PSYCHOMETRIC_UTILS_PATH = PATH + \"psyborgs/psychometric_utils.R\"\n",
    "\n",
    "# filename of pickled results to be analyzed\n",
    "PKL_PATH = \"../results/\" + \"your_results_here.pkl\"\n",
    "\n",
    "# admin_session filename\n",
    "ADMIN_SESSION_PATH = \"../admin_sessions/\" + \\\n",
    "    \"ablation01_ind_big5_9lvls_50desc_admin_session_rating.json\"\n",
    "\n",
    "# save joined IPIP-NEO scores?\n",
    "SAVE_SCORES_FILENAME = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734dfe8",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad809ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(PATH)\n",
    "\n",
    "from psyborgs import score_calculation, survey_bench_lib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dependencies for descriptive statistics\n",
    "import itertools\n",
    "from typing import Union, List\n",
    "\n",
    "# dependencies for R code\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# dependencies for correlation analysis\n",
    "from scipy.stats import spearmanr, pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPID = ['item_preamble_id',\n",
    "        'item_postamble_id',\n",
    "        'response_scale_id',\n",
    "        'response_choice_postamble_id',\n",
    "        'model_id']\n",
    "\n",
    "BFI_SCALE_IDS = [\"BFI-EXT\", \"BFI-AGR\", \"BFI-CON\", \"BFI-NEU\", \"BFI-OPE\"]\n",
    "IPIP_SCALE_IDS = [\"IPIP300-EXT\", \"IPIP300-AGR\", \"IPIP300-CON\", \"IPIP300-NEU\", \"IPIP300-OPE\"]\n",
    "VALIDATION_SCALE_IDS = [\"PA\", \"NA\", \"CSE\", \"CPI\", \"PHYS\", \"VRBL\", \"ANGR\", \"HSTL\", \"ACHV\", \"CONF\", \"SCRT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33bced",
   "metadata": {},
   "source": [
    "## Unpickle Raw Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_response_scores = pd.read_pickle(PKL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70799c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_response_scores.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_raw_response_scores.query(\n",
    "    \"item_postamble_id == 'plk-ipip-0' & item_preamble_id == 'ext0-agr2-con0-neu0-ope0-d36-ev2' & item_id == 'ipip1'\"\n",
    ")\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d0326",
   "metadata": {},
   "source": [
    "## Load Admin Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_session = survey_bench_lib.load_admin_session(\n",
    "    ADMIN_SESSION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ff445",
   "metadata": {},
   "source": [
    "# Score Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt df to match a df with scores for possible continuations\n",
    "df_raw_response_scores['score'] = 1\n",
    "df_raw_response_scores['response_value'] = df_raw_response_scores['model_output'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4225ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score session\n",
    "scored_session_df = score_calculation.score_session(\n",
    "    admin_session, df_raw_response_scores)\n",
    "\n",
    "scored_session_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d57733-e393-4a03-913d-1fda1a3f54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: save scores to disk\n",
    "if SAVE_SCORES_FILENAME:\n",
    "    scored_session_df.to_pickle(SAVE_SCORES_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b31fea",
   "metadata": {},
   "source": [
    "# Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ab311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c924f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_fragments(big5_id, levels=range(1,10)):\n",
    "    \"\"\"Returns list of preamble ID fragments for one domain.\"\"\"\n",
    "    return [f\"{big5_id}{i}\" for i in levels]\n",
    "\n",
    "\n",
    "def get_big5_lvl_fragments(levels=range(1,10)):\n",
    "    \"\"\"Returns list of preamble ID fragments for all Big Five domains.\"\"\"\n",
    "    big5_id_fragments = [\"ext\", \"agr\", \"con\", \"neu\", \"ope\"]\n",
    "    nested_fragments = [get_domain_fragments(big5_id, levels) for big5_id in big5_id_fragments]\n",
    "    preamble_id_fragments = list(itertools.chain(*nested_fragments))\n",
    "    return preamble_id_fragments\n",
    "\n",
    "\n",
    "def subset_one_preamble(df, id_fragment):\n",
    "    return df[df[\"item_preamble_id\"].str.contains(id_fragment)][IPIP_SCALE_IDS]\n",
    "\n",
    "\n",
    "def subset_by_preambles(df, id_fragments):\n",
    "    \"\"\"Subsets data by a given list of item preamble fragments.\"\"\"\n",
    "    preambles = []\n",
    "  \n",
    "    for id_fragment in id_fragments:\n",
    "        preambles.append(subset_one_preamble(df, id_fragment))\n",
    "\n",
    "    return pd.concat(preambles, keys=id_fragments)\n",
    "\n",
    "\n",
    "def describe_by_preambles(\n",
    "    df, id_fragments,\n",
    "    by: Union[str, List[str]]=['median', 'min', 'max', 'std']):\n",
    "    # organize data by preamble_id fragment\n",
    "    df_by_preambles = subset_by_preambles(df, id_fragments)\n",
    "    \n",
    "    # group by preamble_id fragments\n",
    "    df_grouped = df_by_preambles.groupby(level=0)\n",
    "    \n",
    "    # aggregate by specified summary stats\n",
    "    summary = df_grouped.agg(by)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ace18e",
   "metadata": {},
   "source": [
    "### IPIP-NEO-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_session_df[IPIP_SCALE_IDS].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd36162",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(tight_layout=True)\n",
    "scored_session_df[IPIP_SCALE_IDS] \\\n",
    "    .hist(range=[1,5], alpha=1, figsize=(10, 7.5), sharey=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3be1f",
   "metadata": {},
   "source": [
    "## Descriptives by Prompted Personality (In Item Preamble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508172c",
   "metadata": {},
   "source": [
    "### Descriptives of Extremely Low vs. Extremely High Prompts for Each Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big5_domain_lvls = get_big5_lvl_fragments(levels=[1,7])\n",
    "describe_by_preambles(scored_session_df, big5_domain_lvls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9591669",
   "metadata": {},
   "source": [
    "# Quick Validity Check\n",
    "We don't have criterion measures in this run to test for criterion validity, but we can look at the inter-scale correlations of the IPIP-NEO-300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pvalues(df):\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            tmp = df[df[r].notnull() & df[c].notnull()]\n",
    "            pvalues[r][c] = round(pearsonr(tmp[r], tmp[c])[1], 4)\n",
    "    return pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1226e",
   "metadata": {},
   "source": [
    "## IPIP-NEO-300 Intercorrelations\n",
    "EXT should correlate moderately + negatively with NEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9b03e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scored_session_df[IPIP_SCALE_IDS].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p Values\n",
    "calculate_pvalues(scored_session_df[IPIP_SCALE_IDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79741c44",
   "metadata": {},
   "source": [
    "## IPIP-NEO-300 Intercorrelations Across Preamble Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1efc08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scored_session_df \\\n",
    "    .query(f\"item_preamble_id.str.contains('con1')\") \\\n",
    "    [IPIP_SCALE_IDS].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9aafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_session_df \\\n",
    "    .query(f\"item_preamble_id.str.contains('con9')\") \\\n",
    "    [IPIP_SCALE_IDS].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2dc04",
   "metadata": {},
   "source": [
    "# R Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201075ff",
   "metadata": {},
   "source": [
    "## Reliability Functionalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599708e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_r_instance(psychometric_utils_path: str) -> None:\n",
    "    # load R instance\n",
    "    global r\n",
    "    r = robjects.r\n",
    "\n",
    "    # source R script\n",
    "    r['source'](psychometric_utils_path)\n",
    "\n",
    "    # load function(s) within script\n",
    "    global tidyjson_r\n",
    "    tidyjson_r = importr('tidyjson')\n",
    "    # admin_session_to_nested_key_r = robjects.globalenv['admin_session_to_nested_key']\n",
    "    # score_subscale_r = robjects.globalenv['score_subscale']\n",
    "    \n",
    "    global subscale_reliability_r\n",
    "    subscale_reliability_r = robjects.globalenv['subscale_reliability']\n",
    "\n",
    "\n",
    "def load_r_scored_session(scored_session_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Load scored_session_df in R.\"\"\"\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "      scored_session_df_r = robjects.conversion.py2rpy(scored_session_df)\n",
    "    \n",
    "    return scored_session_df_r\n",
    "\n",
    "def compute_reliability_indices_per_scale(admin_session, admin_session_r, scored_session_df_r, **kwargs):   \n",
    "    # create list of scores to be later converted into the output dataframe    \n",
    "    score_list = []\n",
    "\n",
    "    # compute reliability for each scale in an admin_session\n",
    "    # if a particular reliability index can't be estimated, record as NA\n",
    "    for measure_id, measure in admin_session.measures.items():\n",
    "        for scale_id in measure.scales:\n",
    "\n",
    "            # try computing Cronbach's Alpha\n",
    "            try:\n",
    "                alpha = subscale_reliability_r(admin_session_r, scored_session_df_r, measure_id, scale_id, \"alpha\")[0]\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating alpha for measure {measure_id} and scale {scale_id}: {e}\")\n",
    "                alpha = np.nan\n",
    "\n",
    "            # try computing McDonald's Omega\n",
    "            try:\n",
    "                omega = subscale_reliability_r(admin_session_r, scored_session_df_r, measure_id, scale_id, \"omega\")[0]\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating omega for measure {measure_id} and scale {scale_id}: {e}\")\n",
    "                omega = np.nan\n",
    "\n",
    "            # try computing Guttman's Lambda 6\n",
    "            try:\n",
    "                g6 = subscale_reliability_r(admin_session_r, scored_session_df_r, measure_id, scale_id, \"G6\")[0]\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating G6 for measure {measure_id} and scale {scale_id}: {e}\")\n",
    "                g6 = np.nan\n",
    "\n",
    "            # add the above reliability estimates to running score_list\n",
    "            score_list.append([measure_id, scale_id, alpha, omega, g6])\n",
    "\n",
    "    # combine accumulated estimates into one dataframe\n",
    "    reliabilities_df = pd.DataFrame(score_list, columns=['measure_id', 'scale_id', 'alpha', 'omega', 'g6'])\n",
    "    \n",
    "    return reliabilities_df\n",
    "\n",
    "def run_reliability_analysis_in_r(psychometric_utils_path: str,\n",
    "                                  scored_session_df: pd.DataFrame,\n",
    "                                  admin_session_json_path: str) -> pd.DataFrame:\n",
    "    # launch R instance\n",
    "    launch_r_instance(psychometric_utils_path)\n",
    "    \n",
    "    # load admin_session in R\n",
    "    admin_session_r = tidyjson_r.read_json(admin_session_json_path)\n",
    "    \n",
    "    # load scored_session_df into R\n",
    "    scored_session_df_r = load_r_scored_session(scored_session_df)\n",
    "    \n",
    "    # load main admin_session\n",
    "    admin_session = survey_bench_lib.load_admin_session(\n",
    "        admin_session_json_path)\n",
    "    \n",
    "    # compute reliability indices per scale\n",
    "    reliabilities_df = compute_reliability_indices_per_scale(\n",
    "        admin_session, admin_session_r, scored_session_df_r)\n",
    "    \n",
    "    return reliabilities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c3ef0",
   "metadata": {},
   "source": [
    "## Compute Reliability Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23b3b6",
   "metadata": {},
   "source": [
    "Stronger correlations between ordinal intended levels of traits and observed psychometric test scores indicate success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0c20b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_reliability_analysis_in_r(\n",
    "    psychometric_utils_path=PSYCHOMETRIC_UTILS_PATH,\n",
    "    scored_session_df=scored_session_df,\n",
    "    admin_session_json_path=ADMIN_SESSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fd39f",
   "metadata": {},
   "source": [
    "## Compute Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LVL_IDS = [\"lvl-EXT\", \"lvl-AGR\", \"lvl-CON\", \"lvl-NEU\", \"lvl-OPE\"]\n",
    "\n",
    "def main(scores_df):\n",
    "  \"\"\"Calculates correlations between intended trait levels and actual scores.\n",
    "  \n",
    "  Make sure that `variable1` is your level variable, since this function\n",
    "  filters out observations where shaping for a particular trait does not occur\n",
    "  (i.e., where the level is 0).\n",
    "  \"\"\"\n",
    "  # create a list to store the correlations\n",
    "  correlation_data = []\n",
    "\n",
    "  # calculate correlations for each pair of variables\n",
    "  for variable1, variable2 in zip(scores_df.columns[:5], scores_df.columns[5:]):\n",
    "    # retain only data where the intended trait level variable is greater than 0\n",
    "    subset = scores_df[scores_df[variable1] > 0]\n",
    "    \n",
    "    spearman, spearman_p = spearmanr(subset[variable1], subset[variable2])\n",
    "    pearson, pearson_p = pearsonr(subset[variable1], subset[variable2])\n",
    "\n",
    "    # add the correlation coefficient and p-value to the new dataframe\n",
    "    correlation_df = correlation_data.append({\n",
    "      \"Variable1\": variable1,\n",
    "      \"Variable2\": variable2,\n",
    "      \"spearman\": spearman,\n",
    "      \"spearman_p\": spearman_p,\n",
    "      \"pearson\": pearson,\n",
    "      \"pearson_p\": pearson_p\n",
    "    })\n",
    "  \n",
    "  # convert the list to a DataFrame\n",
    "  correlation_df = pd.DataFrame(correlation_data)\n",
    "\n",
    "  # print new dataframe\n",
    "  return(correlation_df)\n",
    "\n",
    "def add_ordinal_levels(df):\n",
    "  scores_df = df[[\"item_preamble_id\"] + IPIP_SCALE_IDS]\n",
    "\n",
    "  scores_df[[\"lvl-EXT\", \"lvl-AGR\", \"lvl-CON\", \"lvl-NEU\", \"lvl-OPE\", \"description_id\", \"instruction_id\"]] = scores_df[\"item_preamble_id\"].str.split('-', expand=True)\n",
    "\n",
    "  print(scores_df[\"lvl-EXT\"])\n",
    "\n",
    "  # replace the values of each lvl- column with only the digits they contain\n",
    "  scores_df.loc[:, \"lvl-EXT\"] = scores_df.loc[:, \"lvl-EXT\"].str[3:].astype(int)\n",
    "  scores_df.loc[:, \"lvl-AGR\"] = scores_df.loc[:, \"lvl-AGR\"].str[3:].astype(int)\n",
    "  scores_df.loc[:, \"lvl-CON\"] = scores_df.loc[:, \"lvl-CON\"].str[3:].astype(int)\n",
    "  scores_df.loc[:, \"lvl-NEU\"] = scores_df.loc[:, \"lvl-NEU\"].str[3:].astype(int)\n",
    "  scores_df.loc[:, \"lvl-OPE\"] = scores_df.loc[:, \"lvl-OPE\"].str[3:].astype(int)\n",
    "\n",
    "  return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = add_ordinal_levels(scored_session_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c51575-6d45-480c-ae69-af072226852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36135e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(df_subset[LVL_IDS + IPIP_SCALE_IDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefb793-bd02-4507-bafc-09ec04d8567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset Spearman correlations only\n",
    "display(main(df_subset[LVL_IDS + IPIP_SCALE_IDS])[[\"spearman\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
